{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwaOSdd3BM9S"
      },
      "source": [
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import re\r\n",
        "from nltk.corpus import wordnet\r\n",
        "import string\r\n",
        "from nltk import pos_tag\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import WhitespaceTokenizer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk import word_tokenize\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "import os\r\n",
        "import pickle\r\n",
        "import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import LSTM\r\n",
        "\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.models import model_from_json\r\n",
        "from keras.models import load_model\r\n",
        "!pip install -q tensorflow-gpu==2.0.0-beta1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB8Z1SffBOc5"
      },
      "source": [
        "df = pd.read_csv(\"Hotel_Reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqUUHvqEBT5C"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPoBQ-sKBUps"
      },
      "source": [
        "df['Review']=df['Negative_Review']+df['Positive_Review']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42_oq4nMBWqm"
      },
      "source": [
        "df['good_review']=df[\"Reviewer_Score\"].apply(lambda x:1 if x>=5 else 0) #X=1 good rev\r\n",
        "                                                                         #X=0 Bad rev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By9E-Zh6BZM6"
      },
      "source": [
        "df_rev=df[['Review','good_review']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auhuyg9ABZT4"
      },
      "source": [
        "df_rev.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuKo_FF-Bf5E"
      },
      "source": [
        "class_count_1, class_count_0 =df_rev['good_review']. value_counts ()\r\n",
        "df_rev_0 = df_rev[df_rev['good_review']==0]\r\n",
        "df_rev_1=df_rev[df_rev['good_review']==1]\r\n",
        "print(class_count_1)\r\n",
        "print(class_count_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVgLz-t4Bgr4"
      },
      "source": [
        "df_rev_1_under = df_rev_1.sample (class_count_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXmR-25IBkK5"
      },
      "source": [
        "df_rev_1_under"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhgkr6deBm9T"
      },
      "source": [
        "dataRev = pd.concat ([df_rev_1_under, df_rev_0], axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF_OMt0_Bo7P"
      },
      "source": [
        "labels = dataRev[\"good_review\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYN_45onBqy1"
      },
      "source": [
        "def get_wordnet(pos_tag):\r\n",
        "    if pos_tag.startswith('J'):\r\n",
        "        return wordnet.ADJ\r\n",
        "    elif pos_tag.startswith('V'):\r\n",
        "        return wordnet.VERB\r\n",
        "    elif pos_tag.startswith('N'):\r\n",
        "        return wordnet.NOUN\r\n",
        "    elif pos_tag.startswith('R'):\r\n",
        "        return wordnet.ADV\r\n",
        "    else:\r\n",
        "        return wordnet.NOUN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZugtBZ7CBvuN"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "def clean_rev(rev):\r\n",
        "    # lower text\r\n",
        "    rev = rev.lower()\r\n",
        "    rev1=rev.split(\" \")\r\n",
        "    # tokenize text and remove puncutation\r\n",
        "    rev = [word.strip(string.punctuation) for word in rev1]\r\n",
        "    # remove words that contain numbers\r\n",
        "    rev = [word for word in rev if not any(c.isdigit() for c in word)]\r\n",
        "    # remove stop words\r\n",
        "    stop_W = stopwords.words('english')\r\n",
        "    rev = [word for word in rev if word not in stop_W]\r\n",
        "    # remove empty tokens\r\n",
        "    rev = [emp for emp in rev if len(emp) > 0]\r\n",
        "    # pos tag text\r\n",
        "    pos_tags = pos_tag(rev)\r\n",
        "    # lemmatize text\r\n",
        "    rev = [WordNetLemmatizer().lemmatize(tag[0], get_wordnet(tag[1])) for tag in pos_tags]\r\n",
        "    # remove words with only one letter\r\n",
        "    rev = [t for t in rev if len(t) >=1]\r\n",
        "    # join all\r\n",
        "    rev = \" \".join(rev)\r\n",
        "    return(rev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIuUfk18BzfR"
      },
      "source": [
        "dataRev['Review'] = dataRev['Review'].transform(lambda x: clean_rev(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UUjZqWxB17j"
      },
      "source": [
        "X = dataRev.drop('good_review', axis=1)\r\n",
        "Y = dataRev[['good_review']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF88wg1-B33P"
      },
      "source": [
        "vocabulary_size = 20000\r\n",
        "tokenizer = Tokenizer(num_words= vocabulary_size)\r\n",
        "tokenizer.fit_on_texts(X['Review'])\r\n",
        "sequences = tokenizer.texts_to_sequences(X['Review'])\r\n",
        "tokenizer.word_index #how to get the word to index mapping \r\n",
        "with open('tokenizer.pickle', 'wb') as handle:\r\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "#maxSize = max(len(seq) for seq in sequences), maxlen=maxSize  # convert a string list to a tensor for a deep learning model\r\n",
        "data = pad_sequences(sequences)\r\n",
        "print(sequences)\r\n",
        "print(data.shape)\r\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYNuaR_GB37g"
      },
      "source": [
        "target=np.array(Y)\r\n",
        "target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2QYB_DdB-I_"
      },
      "source": [
        "acc_per_fold = []\r\n",
        "loss_per_fold = []\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "fold_no = 1\r\n",
        "kf = KFold(n_splits = 5, random_state = 5168, shuffle = False)\r\n",
        "\r\n",
        "\r\n",
        "for train, test in kf.split(data, target):\r\n",
        "    \r\n",
        "    model_lstm = Sequential()\r\n",
        "    model_lstm.add(Embedding(20000, 100, input_length=50))\r\n",
        "    model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\r\n",
        "    model_lstm.add(Dense(1, activation='sigmoid')) \r\n",
        "    model_lstm.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\r\n",
        "    print('------------------------------------------------------------------------')\r\n",
        "    print(f'Training for fold {fold_no} ...')\r\n",
        "    # Fit data to model\r\n",
        "    history = model_lstm .fit(data[train],target[train] , epochs=3,batch_size=180,verbose=2)\r\n",
        "    # Generate generalization metrics\r\n",
        "    scores = model_lstm .evaluate(data[test], target[test], verbose=0)\r\n",
        "    print(f'Score for fold {fold_no}: {model_lstm .metrics_names[0]} of {scores[0]}; {model_lstm .metrics_names[1]} of {scores[1]*100}%')\r\n",
        "    acc_per_fold.append(scores[1] * 100)\r\n",
        "    loss_per_fold.append(scores[0])\r\n",
        "\r\n",
        "    # Increase fold number\r\n",
        "    fold_no = fold_no + 1\r\n",
        "    # == Provide average scores ==\r\n",
        "    print('------------------------------------------------------------------------')\r\n",
        "    print('Score per fold')\r\n",
        "    for i in range(0, len(acc_per_fold)):\r\n",
        "        \r\n",
        "        print('------------------------------------------------------------------------')\r\n",
        "        print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\r\n",
        "        print('------------------------------------------------------------------------')\r\n",
        "        print('Average scores for all folds:')\r\n",
        "        print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\r\n",
        "        print(f'> Loss: {np.mean(loss_per_fold)}')\r\n",
        "        print('------------------------------------------------------------------------')\r\n",
        "model_lstm.save('model.h5', history)\r\n",
        "\r\n",
        "print(\"model is created\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}